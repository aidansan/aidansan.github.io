<html>
    <head>
        <link rel="stylesheet" href="../spectre/spectre.min.css">
        <link rel="stylesheet" href="../spectre/spectre-exp.min.css">
        <link rel="stylesheet" href="../spectre/spectre-icons.min.css">
        <link rel="stylesheet" href="../spectre/custom-styles.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script>
            MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
              },
              svg: {
                fontCache: 'global'
              }
            };
            </script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
    <div class="main-box">
        <header class="navbar">
            <section class="navbar-section">
              <a href="../index.html" class="navbar-brand mr-2">Aidan San</a>
              <a href="https://drive.google.com/file/d/1ngiijG6VV_aL_vizcv0KuTGDHxznax_E/view?usp=sharing" class="btn btn-link">CV</a>
              <a href="https://github.com/aidansan" class="btn btn-link">GitHub</a>
              <a href="https://scholar.google.com/citations?hl=en&user=atZI-McAAAAJ" class="btn btn-link">Google Scholar</a>
              <a href="../blog.html" class="btn btn-link">Blog</a>
            </section>
          </header>
      <h1>Blog - Mutual Information</h1>
      <div>
            $$I(X;Y) = D_{KL}(P_{(X, Y)} \vert\vert\ P_X \cdot P_Y)$$
            $$D_{KL}(P \vert\vert Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)}\right)$$
          
          <p>If the variables are independent then $P_{(X, Y)} = P_X \cdot P_Y$. Thus the KL divergence will be zero so the mutual information will be zero.</p>
          <p>On the other hand consider the other most extreme case, such that $P(X) = P(Y)$. </p>
          <p>
          Then this table represents $P(X, Y)$:
          <table class="table">
            <thead>
              <tr>
                <th></th>
                <th>X=0</th>
                <th>X=1</th>
              </tr>
            </thead><br>
            <tbody>
              <tr>
                <td><b>Y=0</b></td>
                <td>.5</td>
                <td>0</td>
              </tr>
              <tr>
                <td><b>Y=1</b></td>
                <td>0</td>
                <td>.5</td>
              </tr>
            </tbody>
          </table>
          </p>
          <p>
          And this table represents $P_X \cdot P_Y$:
          <table class="table">
            <thead>
              <tr>
                <th></th>
                <th>X=0</th>
                <th>X=1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>Y=0</b></td>
                <td>.25</td>
                <td>.25</td>
              </tr>
              <tr>
                <td><b>Y=1</b></td>
                <td>.25</td>
                <td>.25</td>
              </tr>
            </tbody>
          </table>
          </p>
          <p>
          Then we only need to consider when the the joint distribution ($P_{(X, Y)}$) is not equal to zero. 
          Thus we get $.5 \log \frac{.5}{.25} + .5 \log \frac{.5}{.25} = .5 \log 2 + .5 \log 2 = 1 $. So mutual information can sort of be thought of as how frequently the values of the two variables co-occur or how (in)dependent the two variables are.
          </p>
          <h4>Citations</h4>
          <ul>
          <li>https://en.wikipedia.org/wiki/Joint_probability_distribution</li>
          <li>https://en.wikipedia.org/wiki/Mutual_information</li>
          <li>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</li>
          </ul>
        </div>
    </div>
</body>
</html>